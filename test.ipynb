{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "\n",
    "from utils.data_util import *\n",
    "from utils.train_util import *\n",
    "from torch.utils.data import DataLoader\n",
    "from copy import deepcopy\n",
    "\n",
    "torch.set_printoptions(\n",
    "    precision=2,\n",
    "    threshold=1000,\n",
    "    edgeitems=5,\n",
    "    linewidth=1000,\n",
    "    sci_mode=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "train_dataset, test_dataset, c, h, w = get_dataset('cifar10')\n",
    "net1 = CNN(h, w, c, num_classes=10)\n",
    "net2 = LeNet5(h, w, c, num_classes=10)\n",
    "net3 = torchvision.models.resnet18(weights=None, num_classes=10)\n",
    "model1 = net1.cuda()\n",
    "model2 = net2.cuda()\n",
    "model3 = net3.cuda()\n",
    "trainloader = DataLoader(\n",
    "    dataset=train_dataset,\n",
    "    batch_size=160,\n",
    "    shuffle=True,\n",
    "    pin_memory=True,\n",
    "    num_workers=8)\n",
    "testloader = DataLoader(\n",
    "    dataset=test_dataset,\n",
    "    batch_size=160,\n",
    "    shuffle=True,\n",
    "    pin_memory=True,\n",
    "    num_workers=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_func = torch.nn.CrossEntropyLoss().cuda()\n",
    "model1.train()\n",
    "model2.train()\n",
    "model3.train()\n",
    "optimizer1 = torch.optim.Adam(model1.parameters())\n",
    "optimizer2 = torch.optim.Adam(model2.parameters())\n",
    "optimizer3 = torch.optim.Adam(model3.parameters())\n",
    "for i in range(10):\n",
    "    loss_ = []\n",
    "    for data, target in trainloader:\n",
    "        data_device = data.cuda()\n",
    "        optimizer1.zero_grad()\n",
    "        optimizer2.zero_grad()\n",
    "        optimizer3.zero_grad()\n",
    "        output1 = model1(data_device)\n",
    "        output2 = model2(data_device)\n",
    "        output3 = model3(data_device)\n",
    "        loss1 = loss_func(output1, target.cuda())\n",
    "        loss2 = loss_func(output2, target.cuda())\n",
    "        loss3 = loss_func(output3, target.cuda())\n",
    "        loss1.backward()\n",
    "        loss2.backward()\n",
    "        loss3.backward()\n",
    "        optimizer1.step()\n",
    "        optimizer2.step()\n",
    "        optimizer3.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = get_args()\n",
    "args.device = 'cuda'\n",
    "\n",
    "server_client = [[0, 1, 2], [3, 4, 5], [6, 7, 8]]\n",
    "neighbor_server = [[1], [2], [0]]\n",
    "all_client = [i for i in range(args.num_all_client)]\n",
    "all_server = [i for i in range(args.num_all_server)]\n",
    "num_server_client = args.num_all_client // args.num_all_server\n",
    "\n",
    "train_dataset_o, test_dataset_o, c, h, w = get_dataset(args.dataset)\n",
    "target_list = {0: [0, 1, 2], 1: [3, 4, 5], 2: [6, 7, 8, 9]}\n",
    "num_target, train_dataloader, validate_dataloader = split_dataset(\n",
    "    train_dataset_o, target_list, args)\n",
    "[public_dataset, test_dataset] = split_parts_random(\n",
    "    test_dataset_o, [args.num_public_data, int(len(test_dataset_o)) - args.num_public_data])\n",
    "public_dataloader = DataLoader(\n",
    "    dataset=public_dataset,\n",
    "    batch_size=args.batch_size,\n",
    "    shuffle=True,\n",
    "    pin_memory=True,\n",
    "    num_workers=args.num_workers)\n",
    "test_dataloader = DataLoader(\n",
    "    dataset=test_dataset,\n",
    "    batch_size=args.batch_size,\n",
    "    pin_memory=True,\n",
    "    num_workers=args.num_workers)\n",
    "\n",
    "client_model = [list_same_term(3, model1), list_same_term(3, model2), list_same_term(3, model3)]\n",
    "server_model = [model1, model2, model3]\n",
    "client_accuracy = list_same_term(args.num_all_client)\n",
    "validate_accuracy = list_same_term(args.num_all_client)\n",
    "client_loss = deepcopy(client_accuracy)\n",
    "weight_server = list_same_term(args.num_all_server, 1/args.num_all_server)\n",
    "weight_list = list_same_term(args.num_all_server, weight_server)\n",
    "\n",
    "# %% 模型训练\n",
    "keys = ['server_model',\n",
    "        'train_dataloader',\n",
    "        'test_dataloader',\n",
    "        'validate_dataloader',\n",
    "        'public_dataloader',\n",
    "        'num_target',\n",
    "        'client_accuracy',\n",
    "        'client_loss',\n",
    "        'validate_accuracy',\n",
    "        'weight_list',\n",
    "        'weight_server',\n",
    "        'server_client',\n",
    "        'all_server',\n",
    "        'client_model',\n",
    "        'target_list',\n",
    "        'public_dataset']\n",
    "values = [server_model,\n",
    "          train_dataloader,\n",
    "          test_dataloader,\n",
    "          validate_dataloader,\n",
    "          public_dataloader,\n",
    "          num_target,\n",
    "          client_accuracy,\n",
    "          client_loss,\n",
    "          validate_accuracy,\n",
    "          weight_list,\n",
    "          weight_server,\n",
    "          server_client,\n",
    "          all_server,\n",
    "          client_model,\n",
    "          target_list,\n",
    "          public_dataset]\n",
    "args_train = dict(zip(keys, values))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: [], 1: [], 2: [], 3: [], 4: [], 5: [], 6: [], 7: [], 8: [], 9: []}\n",
      "{0: [0], 1: [0], 2: [0], 3: [1], 4: [1], 5: [1], 6: [2], 7: [2], 8: [2], 9: [2]}\n",
      "10000\n"
     ]
    }
   ],
   "source": [
    "server_model = [model1.eval(), model2.eval(), model3.eval()]\n",
    "\n",
    "def distill_public(args, args_train):\n",
    "    server_model = {}\n",
    "    for server in args_train['all_server']:\n",
    "        server_model[server] = args_train['server_model'][server].eval()\n",
    "    dataset_ = []\n",
    "    target_logits = {}\n",
    "    for i, (data, target) in enumerate(args_train['public_dataset']):\n",
    "        data_device = data.to(args.device)\n",
    "        data_ = torch.unsqueeze(data_device, dim=0)\n",
    "        logits = torch.zeros([1, 10]).to(args.device)\n",
    "        num_target_servers = 0\n",
    "        for server in args_train['all_server']:\n",
    "            if target in args_train['target_list'][server]:\n",
    "                logits += server_model[server](data_)\n",
    "                num_target_servers += 1\n",
    "        logits = logits / num_target_servers\n",
    "        dataset_.append((data, i))\n",
    "        target_logits[i] = (target, logits)\n",
    "    return dataset_, target_logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'torch.utils.data.dataloader.DataLoader'>\n",
      "<class 'torch.Tensor'> <class 'torch.Tensor'> tensor([3150, 3305, 6479, 9536,  539, 6011,  715, 1457, 3301, 3087, 9503, 4522, 6461, 5851, 3707, 5580, 2413, 1108, 9043, 8149, 4837, 8376, 6903, 7396, 8072, 2541, 2226, 4346, 1973, 3891, 1862, 1746, 9097, 3015,  595, 5264, 6971, 5999, 4055, 3208,  375, 9180, 2679, 3298, 6545, 2891, 4135, 5979, 2561, 8473, 1986, 8949,  931, 9679, 8281, 2673, 7734, 8743, 7407, 5518, 1332, 7029, 8797, 7263, 1389, 7220, 9578, 3070, 2108, 2700, 9059, 3363, 4888, 6527, 3088, 8484, 5172, 1992, 3299, 2689, 8205, 9990, 7633, 2020, 7883, 6617, 6935, 8764,  149, 6378, 2180, 9068, 4510, 2457, 9236, 5973,  637, 9831, 9695, 2903, 5640, 9761, 3772,  934, 4580, 6434, 8115, 5503, 5435,  391, 6582, 3399, 7231, 6953, 8911, 7067, 8011, 5299, 5892, 2398, 1946, 9410, 1681, 9505, 1200, 4186, 7476, 5965, 6702, 3034, 7531,  970, 2396, 2471, 7933, 4382, 5199, 5094, 3811, 9670, 1116, 3413, 9252, 4009, 4630, 1315, 3187, 3218, 9074, 9813, 4577,   46, 9851, 7698, 3653, 9581, 1432, 8931, 3101,  555])\n",
      "torch.Size([160, 3, 32, 32]) torch.Size([160])\n",
      "1\n",
      "tensor([[-0.44,  2.96, -1.72, -2.19, -2.98, -2.66, -1.26, -2.78, -0.07,  2.28]], device='cuda:0', grad_fn=<DivBackward0>)\n"
     ]
    }
   ],
   "source": [
    "dataset_, target_logits = distill_public(public_dataset, target_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([])\n",
      "torch.Size([3])\n",
      "torch.Size([80, 10]) torch.Size([160])\n"
     ]
    }
   ],
   "source": [
    "a = torch.tensor([])\n",
    "print(a)\n",
    "b = torch.cat((a, torch.tensor([1, 2, 3])))\n",
    "print(b.shape)\n",
    "print(output1.shape, target.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1, 0], dtype=torch.int32)\n"
     ]
    }
   ],
   "source": [
    "a = torch.zeros([2])\n",
    "b = torch.ones([1])\n",
    "a[0] = b\n",
    "a = a.type(dtype=torch.int)\n",
    "print(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([160, 10]) torch.float32\n",
      "torch.Size([160, 3, 32, 32]) torch.float32\n",
      "torch.Size([160]) torch.int64\n"
     ]
    }
   ],
   "source": [
    "model = deepcopy(model1).train()\n",
    "optimizer = torch.optim.Adam(model.parameters())\n",
    "for i in range(10):\n",
    "    loss_ = []\n",
    "    for data, target in trainloader:\n",
    "        data_device = data.cuda()\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data_device)\n",
    "        loss = loss_func(output, target.cuda())\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        print(output.shape, output.dtype)\n",
    "        print(data.shape, data.dtype)\n",
    "        print(target.shape, target.dtype)\n",
    "        break\n",
    "    break"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
