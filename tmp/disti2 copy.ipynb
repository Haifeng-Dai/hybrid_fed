{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "import copy\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "from utils.data_util import *\n",
    "from utils.model_util import LeNet5\n",
    "\n",
    "torch.set_printoptions(precision=2,\n",
    "                       threshold=1000,\n",
    "                       edgeitems=5,\n",
    "                       linewidth=1000,\n",
    "                       sci_mode=False)\n",
    "# 是否使用显卡加速\n",
    "if torch.cuda.is_available():\n",
    "    device = 'cuda'\n",
    "    if torch.backends.cudnn.is_available():\n",
    "        torch.backends.cudnn.enabled = True\n",
    "        torch.backends.cudnn.benchmark = True\n",
    "elif torch.backends.mps.is_available():\n",
    "    device = 'mps'\n",
    "else:\n",
    "    device = 'cpu'\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset, test_dateset, c, h, w = get_dataset()\n",
    "DataSplit = SplitData(train_dataset)\n",
    "[teacher_dataset, student_dataset, distill_dataset, test_dataset] = DataSplit.all_iid(4, 3200)\n",
    "num_target = DataSplit.num_target\n",
    "\n",
    "train_dataloder = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "test_dataloder = DataLoader(test_dateset, batch_size=32, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoches:0,accurate=0.8852999806404114\n",
      "epoches:1,accurate=0.9167999625205994\n",
      "epoches:2,accurate=0.9299999475479126\n",
      "epoches:3,accurate=0.9434999823570251\n",
      "epoches:4,accurate=0.949999988079071\n",
      "epoches:5,accurate=0.9596999883651733\n"
     ]
    }
   ],
   "source": [
    "model = LeNet5(h, w, c, num_target)\n",
    "# model = Teacher_model()\n",
    "model = model.to(device)\n",
    "\n",
    "# 损失函数和优化器\n",
    "loss_function = nn.CrossEntropyLoss()\n",
    "optim = torch.optim.Adam(model.parameters(), lr=0.0001)\n",
    "\n",
    "epoches = 6\n",
    "for epoch in range(epoches):\n",
    "    model.train()\n",
    "    for image, label in train_dataloder:\n",
    "        image, label = image.to(device), label.to(device)\n",
    "        optim.zero_grad()\n",
    "        out = model(image)\n",
    "        loss = loss_function(out, label)\n",
    "        loss.backward()\n",
    "        optim.step()\n",
    "\n",
    "    model.eval()\n",
    "    num_correct = 0\n",
    "    num_samples = 0\n",
    "    with torch.no_grad():\n",
    "        for image, label in test_dataloder:\n",
    "            image = image.to(device)\n",
    "            label = label.to(device)\n",
    "            out = model(image)\n",
    "            pre = out.max(1).indices\n",
    "            num_correct += (pre == label).sum()\n",
    "            num_samples += pre.size(0)\n",
    "        acc = (num_correct/num_samples).item()\n",
    "\n",
    "    model.train()\n",
    "    print(\"epoches:{},accurate={}\".format(epoch, acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "T:0.2, epoches:0, accurate=0.752\n",
      "T:0.2, epoches:1, accurate=0.744\n",
      "T:0.2, epoches:2, accurate=0.886\n",
      "T:0.2, epoches:3, accurate=0.901\n",
      "T:0.2, epoches:4, accurate=0.671\n",
      "T:0.4, epoches:0, accurate=0.740\n",
      "T:0.4, epoches:1, accurate=0.725\n",
      "T:0.4, epoches:2, accurate=0.738\n",
      "T:0.4, epoches:3, accurate=0.827\n",
      "T:0.4, epoches:4, accurate=0.877\n",
      "T:0.6, epoches:0, accurate=0.800\n",
      "T:0.6, epoches:1, accurate=0.811\n",
      "T:0.6, epoches:2, accurate=0.880\n",
      "T:0.6, epoches:3, accurate=0.815\n",
      "T:0.6, epoches:4, accurate=0.780\n",
      "T:0.8, epoches:0, accurate=0.722\n",
      "T:0.8, epoches:1, accurate=0.738\n",
      "T:0.8, epoches:2, accurate=0.810\n",
      "T:0.8, epoches:3, accurate=0.728\n",
      "T:0.8, epoches:4, accurate=0.743\n",
      "T:1.0, epoches:0, accurate=0.727\n",
      "T:1.0, epoches:1, accurate=0.748\n",
      "T:1.0, epoches:2, accurate=0.804\n",
      "T:1.0, epoches:3, accurate=0.794\n",
      "T:1.0, epoches:4, accurate=0.795\n",
      "T:1.2, epoches:0, accurate=0.802\n",
      "T:1.2, epoches:1, accurate=0.873\n",
      "T:1.2, epoches:2, accurate=0.736\n",
      "T:1.2, epoches:3, accurate=0.783\n",
      "T:1.2, epoches:4, accurate=0.800\n",
      "T:1.4, epoches:0, accurate=0.795\n",
      "T:1.4, epoches:1, accurate=0.796\n",
      "T:1.4, epoches:2, accurate=0.735\n",
      "T:1.4, epoches:3, accurate=0.871\n",
      "T:1.4, epoches:4, accurate=0.805\n",
      "T:1.6, epoches:0, accurate=0.738\n",
      "T:1.6, epoches:1, accurate=0.576\n",
      "T:1.6, epoches:2, accurate=0.737\n",
      "T:1.6, epoches:3, accurate=0.657\n",
      "T:1.6, epoches:4, accurate=0.802\n",
      "T:1.8, epoches:0, accurate=0.857\n",
      "T:1.8, epoches:1, accurate=0.858\n",
      "T:1.8, epoches:2, accurate=0.747\n",
      "T:1.8, epoches:3, accurate=0.801\n",
      "T:1.8, epoches:4, accurate=0.737\n",
      "T:2.0, epoches:0, accurate=0.657\n",
      "T:2.0, epoches:1, accurate=0.647\n",
      "T:2.0, epoches:2, accurate=0.799\n",
      "T:2.0, epoches:3, accurate=0.733\n",
      "T:2.0, epoches:4, accurate=0.724\n",
      "T:2.2, epoches:0, accurate=0.799\n",
      "T:2.2, epoches:1, accurate=0.718\n",
      "T:2.2, epoches:2, accurate=0.730\n",
      "T:2.2, epoches:3, accurate=0.660\n",
      "T:2.2, epoches:4, accurate=0.732\n",
      "T:2.4, epoches:0, accurate=0.802\n",
      "T:2.4, epoches:1, accurate=0.849\n",
      "T:2.4, epoches:2, accurate=0.794\n",
      "T:2.4, epoches:3, accurate=0.726\n",
      "T:2.4, epoches:4, accurate=0.655\n",
      "T:2.6, epoches:0, accurate=0.722\n",
      "T:2.6, epoches:1, accurate=0.572\n",
      "T:2.6, epoches:2, accurate=0.658\n",
      "T:2.6, epoches:3, accurate=0.732\n",
      "T:2.6, epoches:4, accurate=0.660\n",
      "T:2.8, epoches:0, accurate=0.728\n",
      "T:2.8, epoches:1, accurate=0.551\n",
      "T:2.8, epoches:2, accurate=0.782\n",
      "T:2.8, epoches:3, accurate=0.794\n",
      "T:2.8, epoches:4, accurate=0.658\n",
      "T:3.0, epoches:0, accurate=0.457\n",
      "T:3.0, epoches:1, accurate=0.563\n",
      "T:3.0, epoches:2, accurate=0.477\n",
      "T:3.0, epoches:3, accurate=0.793\n",
      "T:3.0, epoches:4, accurate=0.453\n",
      "T:3.2, epoches:0, accurate=0.565\n",
      "T:3.2, epoches:1, accurate=0.476\n",
      "T:3.2, epoches:2, accurate=0.643\n",
      "T:3.2, epoches:3, accurate=0.639\n",
      "T:3.2, epoches:4, accurate=0.460\n",
      "T:3.4, epoches:0, accurate=0.731\n",
      "T:3.4, epoches:1, accurate=0.642\n",
      "T:3.4, epoches:2, accurate=0.481\n",
      "T:3.4, epoches:3, accurate=0.367\n",
      "T:3.4, epoches:4, accurate=0.475\n",
      "T:3.6, epoches:0, accurate=0.537\n",
      "T:3.6, epoches:1, accurate=0.459\n",
      "T:3.6, epoches:2, accurate=0.370\n",
      "T:3.6, epoches:3, accurate=0.702\n",
      "T:3.6, epoches:4, accurate=0.647\n",
      "T:3.8, epoches:0, accurate=0.551\n",
      "T:3.8, epoches:1, accurate=0.255\n",
      "T:3.8, epoches:2, accurate=0.476\n",
      "T:3.8, epoches:3, accurate=0.548\n",
      "T:3.8, epoches:4, accurate=0.644\n",
      "T:4.0, epoches:0, accurate=0.367\n",
      "T:4.0, epoches:1, accurate=0.447\n",
      "T:4.0, epoches:2, accurate=0.554\n",
      "T:4.0, epoches:3, accurate=0.639\n",
      "T:4.0, epoches:4, accurate=0.457\n",
      "T:4.2, epoches:0, accurate=0.446\n",
      "T:4.2, epoches:1, accurate=0.451\n",
      "T:4.2, epoches:2, accurate=0.365\n",
      "T:4.2, epoches:3, accurate=0.458\n",
      "T:4.2, epoches:4, accurate=0.559\n",
      "T:4.4, epoches:0, accurate=0.476\n",
      "T:4.4, epoches:1, accurate=0.348\n",
      "T:4.4, epoches:2, accurate=0.365\n",
      "T:4.4, epoches:3, accurate=0.499\n",
      "T:4.4, epoches:4, accurate=0.364\n",
      "T:4.6, epoches:0, accurate=0.445\n",
      "T:4.6, epoches:1, accurate=0.452\n",
      "T:4.6, epoches:2, accurate=0.361\n",
      "T:4.6, epoches:3, accurate=0.172\n",
      "T:4.6, epoches:4, accurate=0.448\n",
      "T:4.8, epoches:0, accurate=0.441\n",
      "T:4.8, epoches:1, accurate=0.448\n",
      "T:4.8, epoches:2, accurate=0.447\n",
      "T:4.8, epoches:3, accurate=0.365\n",
      "T:4.8, epoches:4, accurate=0.362\n",
      "T:5.0, epoches:0, accurate=0.360\n",
      "T:5.0, epoches:1, accurate=0.179\n",
      "T:5.0, epoches:2, accurate=0.448\n",
      "T:5.0, epoches:3, accurate=0.442\n",
      "T:5.0, epoches:4, accurate=0.182\n",
      "T:5.2, epoches:0, accurate=0.273\n",
      "T:5.2, epoches:1, accurate=0.181\n",
      "T:5.2, epoches:2, accurate=0.180\n",
      "T:5.2, epoches:3, accurate=0.364\n",
      "T:5.2, epoches:4, accurate=0.442\n",
      "T:5.4, epoches:0, accurate=0.358\n",
      "T:5.4, epoches:1, accurate=0.272\n",
      "T:5.4, epoches:2, accurate=0.267\n",
      "T:5.4, epoches:3, accurate=0.176\n",
      "T:5.4, epoches:4, accurate=0.180\n",
      "T:5.6, epoches:0, accurate=0.179\n",
      "T:5.6, epoches:1, accurate=0.181\n",
      "T:5.6, epoches:2, accurate=0.181\n",
      "T:5.6, epoches:3, accurate=0.176\n",
      "T:5.6, epoches:4, accurate=0.269\n",
      "T:5.8, epoches:0, accurate=0.348\n",
      "T:5.8, epoches:1, accurate=0.358\n",
      "T:5.8, epoches:2, accurate=0.181\n",
      "T:5.8, epoches:3, accurate=0.097\n",
      "T:5.8, epoches:4, accurate=0.182\n",
      "T:6.0, epoches:0, accurate=0.271\n",
      "T:6.0, epoches:1, accurate=0.343\n",
      "T:6.0, epoches:2, accurate=0.178\n",
      "T:6.0, epoches:3, accurate=0.266\n",
      "T:6.0, epoches:4, accurate=0.179\n",
      "T:6.2, epoches:0, accurate=0.178\n",
      "T:6.2, epoches:1, accurate=0.176\n",
      "T:6.2, epoches:2, accurate=0.178\n",
      "T:6.2, epoches:3, accurate=0.181\n",
      "T:6.2, epoches:4, accurate=0.266\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 27\u001b[0m\n\u001b[1;32m     24\u001b[0m     ditillation_loss \u001b[38;5;241m=\u001b[39m soft_loss(\n\u001b[1;32m     25\u001b[0m         F\u001b[38;5;241m.\u001b[39msoftmax(out\u001b[38;5;241m/\u001b[39mT, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m), F\u001b[38;5;241m.\u001b[39msoftmax(teacher_output\u001b[38;5;241m/\u001b[39mT, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m))\n\u001b[1;32m     26\u001b[0m     loss_all \u001b[38;5;241m=\u001b[39m loss \u001b[38;5;241m*\u001b[39m alpha \u001b[38;5;241m+\u001b[39m ditillation_loss \u001b[38;5;241m*\u001b[39m (\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m-\u001b[39m alpha)\n\u001b[0;32m---> 27\u001b[0m     \u001b[43mloss_all\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     28\u001b[0m     optim\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m     30\u001b[0m model\u001b[38;5;241m.\u001b[39meval()\n",
      "File \u001b[0;32m~/app/miniconda3/envs/fl/lib/python3.11/site-packages/torch/_tensor.py:492\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    482\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    483\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    484\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    485\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    490\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    491\u001b[0m     )\n\u001b[0;32m--> 492\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    493\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    494\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/app/miniconda3/envs/fl/lib/python3.11/site-packages/torch/autograd/__init__.py:251\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    246\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    248\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    249\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    250\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 251\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    252\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    253\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    254\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    255\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    256\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    257\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    258\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    259\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# 开始进行知识蒸馏算法\n",
    "teacher_model = copy.deepcopy(model)\n",
    "teacher_model.eval()\n",
    "# model = Student_model()\n",
    "hard_loss = nn.CrossEntropyLoss()\n",
    "alpha = 0\n",
    "soft_loss = nn.KLDivLoss(reduction=\"batchmean\")\n",
    "epoches = 5\n",
    "# 蒸馏温度\n",
    "for t in range(1, 50):\n",
    "    T = t * 0.2\n",
    "    for epoch in range(epoches):\n",
    "        model = LeNet5(h, w, c, num_target)\n",
    "        model = model.to(device)\n",
    "        model.train()\n",
    "        optim = torch.optim.Adam(model.parameters(), lr=0.0001)\n",
    "        for image, label in train_dataloder:\n",
    "            image, label = image.to(device), label.to(device)\n",
    "            with torch.no_grad():\n",
    "                teacher_output = teacher_model(image)\n",
    "            optim.zero_grad()\n",
    "            out = model(image)\n",
    "            loss = hard_loss(out, label)\n",
    "            ditillation_loss = soft_loss(\n",
    "                F.softmax(out/T, dim=1), F.softmax(teacher_output/T, dim=1))\n",
    "            loss_all = loss * alpha + ditillation_loss * (1 - alpha)\n",
    "            loss_all.backward()\n",
    "            optim.step()\n",
    "\n",
    "        model.eval()\n",
    "        num_correct = 0\n",
    "        num_samples = 0\n",
    "        with torch.no_grad():\n",
    "            for image, label in test_dataloder:\n",
    "                image = image.to(device)\n",
    "                label = label.to(device)\n",
    "                out = model(image)\n",
    "                pre = out.max(1).indices\n",
    "                num_correct += (pre == label).sum()\n",
    "                num_samples += pre.size(0)\n",
    "            acc = (num_correct/num_samples).item()\n",
    "        print(\"T:{:.1f}, epoches:{}, accurate={:.3f}\".format(T, epoch, acc))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
